\documentclass{tufte-handout}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\hypersetup{colorlinks,linkcolor=blue}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} %% texi2dvi ~ bug
\usepackage{tikz} % http://www.texample.net/tikz/examples/tikzdevice-demo/
\usepackage{natbib}
\usepackage{bm}
\newcommand{\code}[1]{{\tt #1}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\Y}{{\mathbf Y}}
\newcommand{\V}{{\mathbf V}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\bmu}{{\bm \mu}}
\newcommand{\X}{\mathbf X}

\title{From logistic to binomial \& Poisson models}
\author{Ben Bolker}
\begin{document}
\maketitle
\bibliographystyle{chicago}

\includegraphics[width=2.64cm,height=0.93cm]{../pix/cc-attrib-nc.png}
\begin{minipage}[b]{3in}
{\tiny Licensed under the Creative Commons 
  attribution-noncommercial license
(\url{http://creativecommons.org/licenses/by-nc/3.0/}).
Please share \& remix noncommercially,
mentioning its origin.}
\end{minipage}

<<opts,echo=FALSE,message=FALSE>>=
library("knitr")
do_tikz <- FALSE
opts_chunk$set(tidy=FALSE,fig.width=6,fig.height=4,fig.position="center",
               dev="pdf")
knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       par(bty="l",las=1)
                   } else { }
               })
library(ggplot2)
theme_set(theme_bw()) 
@ 

Logistic regression is special in some ways:

\begin{itemize}
\item conditional distribution (Bernoulli) is always correct
\item model diagnostics especially hard
\item no possibility of \emph{overdispersion}
\end{itemize}

\section{(Aggregated) binomial regression}

Binomial with $N>1$. Basically the same procedures as logistic regression, \emph{except}:

\begin{itemize}
\item easier to do exploration, diagnostics (data are already aggregated)
\item need to specify response \emph{either} as a two-column matrix: \code{cbind(num\_successes,num\_failures)} \emph{or} (\emph{recommended}) as a proportion with the additional \code{weights} variable giving the total number of trials.
\item need to check for \textbf{overdispersion} (see below)
\end{itemize}
  
Set up an example to use:

<<lizards>>=
lizards <- read.csv("../data/lizards.csv")
## gfrac (= fraction grahami), N (=grahami+opalinus) already defined
lizards <- transform(lizards,
                     time=factor(time,levels=c("early","midday","late")))
g1 <- glm(cbind(grahami,opalinus) ~ height+diameter+light+time,
          lizards, family=binomial)
g2 <- update(g1,  gfrac ~ ., weight=N)
## or

## check answers are the same
all.equal(coef(g1),coef(g2))
@

\section{Model diagnostics}

\begin{description}
\item[Graphical] plot computed diagnostic summaries and/or transformations
of residuals to highlight particular classes of model deviations
\item[Formal]
\begin{itemize}
\item compute an overall goodness-of-fit statistic with a known null
distribution
\item embed the model in a larger parametric family; compare
via likelihood ratio test (consider exact or ``round'' alternative).
May use \emph{score test} or single-step update for computational
efficiency.
<<quadlogpic,echo=FALSE>>=
## f' = -2*z*exp(-z^2)
## f'' = -2*(exp(-z^2)-2*z^2*exp(-z^2)) = -2*exp(-z^2)*(1-2*z^2)
zlin <- function(z,a) -2*(z-a)*exp(-(z-a)^2)
zquad <- function(z,a) -2*exp(-(z-a)^2)*(1-2*(z-a)^2)
## c(zlin(1,1),zquad(1,1))
## c(zlin(0,1),zquad(0,1))
L0 <- exp(-1)
L1 <- exp(0)
curve(exp(-(x-1)^2),from=-1,to=1.5,ylab="log-likelihood",ylim=c(0,1.5),axes=FALSE)
box(bty="l")
wfun <- function(x) zquad(1,1)/2*(x-1)^2+1
sfun <- function(x) exp(-1)+zlin(0,1)*x+zquad(0,1)/2*x^2
curve(wfun(x),add=TRUE,lty=2,col=2)
curve(sfun(x),add=TRUE,lty=2,col=4)
abline(v=c(0,1),col="gray")
u1 <- par("usr")[1]
segments(u1,L0,0,L0,lty=3)
segments(u1,L1,1,L1,lty=3)
segments(u1,wfun(0),0,wfun(0),col=2,lty=3)
segments(u1,sfun(1),1,sfun(1),col=4,lty=3)
axis(side=1,at=c(0,1),label=c("$H_0$","$H_1$"))
axis(side=2,at=c(L0,L1),label=c("$\\ell_0$","$\\ell_1$"))
arrows(u1+0.25,L0,u1+0.25,L1,code=3)
text(u1+0.3,(L0+L1)/2,"LRT")
arrows(u1+0.35,wfun(0),u1+0.35,L1,code=3,col=2)
text(u1+0.4,(wfun(0)+L1)/2,"Wald",col=2)
arrows(u1+0.55,L0,u1+0.55,sfun(1),code=3,col=4)
text(u1+0.6,(L0+sfun(1))/2,"score",col=4)
@
\citep{fears_reminder_1996,pawitan_reminder_2000}
\end{itemize}
\end{description}
%``General score tests for regression models incorporating 'robust' variance estimates'' (Clayton and Howson, \url{http://www.stata.com/meeting/9uk/abstracts.html}, 9th Stata UK User Group meeting, May 2003)
%\includegraphics[width=5in]{nested_tests.png}
%(from \href{http://www.ats.ucla.edu/stat/mult_pkg/faq/general/nested_tests.htm}{UCLA stats FAQ}, ultimately from Fox 1997)

\subsection{Residuals}

Different types of residuals (\code{?residuals.glm},
\code{?rstandard}, \code{?rstudent})
\begin{description}
\item[Raw] $y-\mu$
\item[Deviance] $\text{sign}(y-\mu) \sqrt{w \text{deviance}}$
\item[Pearson] $(y-\mu)/(w \sqrt{V(\mu)})$
\item[Standardized] $(y-\mu) / (\sqrt{V(\mu) (1 - H)}$
\end{description}

Note whether residuals are scaled by (1) variance function,
(2) weights, (3) full variance (i.e. including overdispersion factor $\phi$),
(4) diagonal of \emph{hat matrix} (\code{hatvalues()}).  

(Hat matrix: weighted version of $H=\X(\X^T\X)^{-1} \X^T$:
 maps $\y$ to $\hat \y$, so $h_{ii}$ is the influence of $y_i$ on
$\hat y_i$.  All hat values are identical for linear models with categorical
variables, but not for regression models/GLMs \ldots)

\subsection{Linearity}

\begin{itemize}
\item (Deviance) residual vs. fitted plot
\item (Deviance) residuals vs. individual predictors, or
combinations of predictors
\item link test \cite{pregibon_goodness_1980}; try adding
a quadratic term in the linear predictor, see if it fits better
\item Adjust by
  \begin{itemize}
    \item changing link function: \code{power()})
    \item adding polynomial or spline terms 
      to individual predictors (\code{poly()},
      \code{splines::ns()})
    \item transforming individual predictors
  \end{itemize}
\end{itemize}

\subsection{Variance function}

\begin{itemize}
\item Scale-location plot: $\sqrt{\mbox{abs}(\text{residuals})}$ vs.
  fitted value, or individual parameters, or combinations of parameters. If
  residuals are scaled and there is no overdispersion 
  (see below) then the center is at 1
\item Adjust by 
  \begin{itemize}
    \item fixing some other part of the model
    \item change the variance function
  \end{itemize}
\end{itemize}

\subsection{Distributional assumptions}

The variance function and link function might both
be right, but the model distribution can still be wrong
(e.g. log-Normal vs Gamma, zero-inflation).

\begin{itemize}
\item assessing distributional assumption is hard because it's the
  \emph{conditional} distribution 
\item Q-Q plot (examples): good, but only really
  valid asymptotically (i.e. conditional distribution of
  \emph{individual samples} $\approx$ Normal: e.g.
  $\lambda>5$ for Poisson, $n\text{min}(p,1-p) >5$ for Binomial)
\item alternatives to Q-Q plot, e.g. \citep{hoaglin_poissonness_1980}
  (not really practical)
\item Improved Q-Q plot: \code{mgcv::qq.gam()} \cite{augustin_quantile_2012},
  \code{DHARMa::simulateResiduals()} \cite{hartig_dharma_2018}
\item Adjust by
  \begin{itemize}
  \item alternative distribution (log-Normal/Gamma)
  \item ordinal models
  \item robust models (\code{robustbase::glmrob})
  \end{itemize}
\end{itemize}

\subsection{Influential points}

\code{?influence.measures}

\begin{itemize}
\item Cook's distance (overall influence)
\item leverage
\item Adjust by
  \begin{itemize}
  \item leaving out influential points to see if it makes a difference
  \item robust modeling (\code{robustbase::glmrob})
  \end{itemize}
\end{itemize}


\section{Contraception example \#2}
Contraceptive use data showing the distribution of 1607 currently married and fecund women interviewed in the Fiji Fertility Survey, according to age, education, desire for more children and current use of contraception:
downloaded from [http://data.princeton.edu/wws509/datasets/cuse.dat](http://data.princeton.edu/wws509/datasets/cuse.dat).
<<cusedat,cache=TRUE>>=
cuse <- read.table("../data/cuse.dat",header=TRUE)
@ 

Add convenience variables (proportion and total in each group):
change the \code{education} factor so that ``low'' rather
than ``high'' is the baseline group:
<<cuse2>>=
cuse <- transform(cuse,
                  propUsing=using/(using+notUsing),
                  tot=using+notUsing,
                  education=relevel(education,"low"))
@ 

\code{ggplot} tricks:
\begin{itemize}
  \item use \verb+label_both+ in the
\verb+facet_grid+ specification to get the subplots labelled
by their factor name as well as the level name
\item use \code{aes(x=as.numeric(age))} to convince ggplot
  to connect the factor levels on the $x$ axis with lines;
  use \code{size=0.5} to make the lines a little skinnier
  \end{itemize}
  
<<gg_cuse>>=
(gg1 <- ggplot(cuse,aes(x=age,y=propUsing,size=tot,colour=wantsMore))+
  facet_grid(.~education,labeller=label_both)+
  geom_point(alpha=0.9)+
  geom_line(aes(x=as.numeric(age)),size=0.5))
@ 

We could fit the three-way interaction, but it would be a bit
silly because there would be as many parameters as observations
(this is called a \emph{saturated model}.
It would probably be more sensible to include only two-way interactions:
<<cuse2way,message=FALSE,warning=FALSE>>=
fit2 <- glm(propUsing~(age+education+wantsMore)^2,
            weights=tot,
            family=binomial,
            data=cuse)
@ 

<<cuse_diag,fig.keep="none">>=
plot(fit2)
@

<<cuse_diag_broom, fig.keep="none">>=
library(broom)
cuse2 <- augment(fit2,data=cuse)
ggplot(cuse2,aes(.fitted,.resid))+
    geom_point(aes(size=tot))+
    geom_smooth(aes(weight=tot))  ## weight variable
ggplot(cuse2,aes(.fitted,sqrt(abs(.resid))))+
    geom_point(aes(size=tot))+
    geom_smooth(aes(weight=tot))
@ 

<<cuse_DHARMa, fig.keep="none",cache=TRUE>>=
p1 <- DHARMa::simulateResiduals(fit2,plot=TRUE)
@ 

\section{Overdispersion}

\paragraph{Detection}
\begin{itemize}
\item Variance $>$ expected (e.g. assume variance = mean but variance $>$ mean)
\item Test: $\sum (\text{Pearson residuals})^2 \approx \text{residual df}$
\item More specifically, $\sum r^2 \sim \chi^2_{n-p}$
\item \verb+pchisq(sum(residuals(.,type="pearson")^2),rdf,lower.tail=FALSE)+, or \code{aods3::gof(.)}
\end{itemize}

\paragraph{Meaning}
\begin{itemize}
\item May be caused by poor model \ldots
\item \emph{or} may be ``intrinsic''
\item \textbf{don't worry about overdispersion until other modeling issues are dealt with}
\item overdispersion $>2$ probably means there is a larger problem with the data: check (again) for outliers, obvious lack of fit
\item \textbf{only} relevant for families with fixed variance (binomial, Poisson), and \textbf{not} for Bernoulli responses
\end{itemize}

\paragraph{Solutions}
\begin{itemize}
\item quasi-likelihood $\phi \equiv \sum r^2/(n-p)$: scales all likelihoods by $\phi$, all CI by $\sqrt{\phi}$; \code{family="quasipoisson"}, \code{family="quasibinomial"} in R (? likelihoods ?)
\item compound/conjugate model
  \begin{itemize}
  \item negative binomial (Gamma-Poisson) (via \code{MASS::glm.nb}, \code{glmmTMB})
  \item Beta-Binomial (via \code{glmmTMB}, \code{bbmle}?)
  \end{itemize}
\item link-Normal model:
  GLMM with observation-level random effect (Gaussian on linear predictor scale)
\end{itemize}

In this
<<cuse_gof>>=
aods3::gof(fit2)
@

There do indeed seem to be important two-way interactions:
<<cusedrop>>=
drop1(fit2,test="Chisq")
@ 

<<dw_cuse,message=FALSE>>=
library(dotwhisker)
dwplot(fit2)
@ 


\section{Revisiting the AIDS data}

<<aids1,results="hide",message=FALSE>>=
aids <- read.csv("../data/aids.csv")
aids <- transform(aids,
                  date=year+(quarter-1)/4,
                  index=seq(nrow(aids)))
@

<<aidsfits>>=
g1 <- glm(cases~date, data=aids, family=poisson)
g2 <- update(g1,  . ~ poly(date,2))
@

<<>>=
aods3::gof(g1)
aods3::gof(g2)
@

Looks marginal.

<<quasi>>=
g3 <- update(g2, family=quasipoisson)
@

<<otherfits,eval=FALSE>>=
g4A <- MASS::glm.nb(cases~poly(date,2), data=aids)
g4B <- glmmTMB::glmmTMB(cases~poly(date,2), data=aids, family=nbinom2)
g4C <- glmmTMB::glmmTMB(cases~poly(date,2), data=aids, family=nbinom1)
bbmle::AICtab(poisson=g2,nbinom1=g4C)
@

In this case the fancier model is actually slightly \emph{worse}
according to any criteria we measure \ldots

<<lrt,eval=FALSE>>=
pchisq(-2*logLik(g2)-(-2*logLik(g4C)),lower.tail=FALSE,df=1)
@ 

\bibliography{../glmm}
\end{document}

