\documentclass{tufte-handout}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\hypersetup{colorlinks,linkcolor=blue}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} %% texi2dvi ~ bug
\usepackage{tikz} % http://www.texample.net/tikz/examples/tikzdevice-demo/
\usepackage{natbib}
\usepackage{bm}
\newcommand{\code}[1]{{\tt #1}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\Y}{{\mathbf Y}}
\newcommand{\V}{{\mathbf V}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\bmu}{{\bm \mu}}
\newcommand{\X}{\mathbf X}

\title{Complete separation, regularization, etc.}
\author{Ben Bolker}
\begin{document}
\maketitle
\bibliographystyle{chicago}

\includegraphics[width=2.64cm,height=0.93cm]{../pix/cc-attrib-nc.png}
\begin{minipage}[b]{3in}
{\tiny Licensed under the Creative Commons 
  attribution-noncommercial license
(\url{http://creativecommons.org/licenses/by-nc/3.0/}).
Please share \& remix noncommercially,
mentioning its origin.}
\end{minipage}

<<opts,echo=FALSE,message=FALSE>>=
library("knitr")
opts_chunk$set(tidy=FALSE,fig.width=6,fig.height=4,fig.position="center",
               dev="tikz")
knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       par(bty="l",las=1)
                   } else { }
               })
@

\section{Complete separation}

Some linear combination of predictor variables perfectly separates

\begin{itemize}
\item \code{glm} goes as far as it can, stops --- may or may not
warn you!
<<simsep>>=
x <- runif(25)
y <- ifelse(x<0.5,0,1)
d <- data.frame(x,y)
g0 <- glm(y~x,family=binomial,data=data.frame(x,y))
coef(g0)
@
\item easy to diagnose for small/low-dimensional data sets,
  harder for high-dimensional data sets
\item GLMMs partly take care of this (by handling completely
  separated blocks)
\item \emph{support vector machines}
\item Firth algorithm: bias-reduced logistic regression; modify
  score function; equivalent to imposing \emph{Jeffreys prior} on
  the data (\code{logistf} package).  Prior on $p$ is $\text{Beta}(1/2,1/2)$
<<message=FALSE>>=
library(logistf)
g1 <- logistf(y~x,family=binomial,data=d)
coef(g1)
confint(g1)
@
<<message=FALSE>>=
library(brglm)
g3 <- brglm(y~x,family=binomial,data=d)
coef(g3)
confint(g3)
@
\item \code{bayesglm} function, from the \code{arm} package;
  $t$-distributed priors, default is df=1 (Cauchy)
<<>>=
library(arm)
g2 <- bayesglm(y~x,family=binomial,data=d)
coef(g2)
@
\end{itemize}

We used this in \citep{pasch_interspecific_2013}; we initially
used a GLMM but the variances kept coming out to zero so we
decided to fit a bias-reduced logistic instead.

<<>>=
library("brglm2")
data("endometrial", package = "brglm2")
modML <- glm(HG ~ NV + PI + EH,
             family = binomial("probit"), data = endometrial)
update(modML, method="detect_separation")
cc <- check_infinite_estimates(modML)
matplot(cc,log="y",type="l")
@

<<>>=
update(modML,method="brglmFit")
@ 

\bibliography{glmm}
\end{document}

