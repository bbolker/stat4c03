\documentclass{tufte-handout}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\hypersetup{colorlinks,linkcolor=blue}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} %% texi2dvi ~ bug
\usepackage{tikz} % http://www.texample.net/tikz/examples/tikzdevice-demo/
\usepackage{natbib}
\usepackage{bm}
\newcommand{\code}[1]{{\tt #1}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\Y}{{\mathbf Y}}
\newcommand{\V}{{\mathbf V}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\bmu}{{\bm \mu}}
\newcommand{\X}{\mathbf X}
\newcommand{\forkpath}{\raisebox{-3pt}{\includegraphics[width=0.4cm]{../pix/fp_icon.png}}}

\title{Logistic regression}
\author{Ben Bolker}
\begin{document}
\maketitle
\bibliographystyle{chicago}

\includegraphics[width=2.64cm,height=0.93cm]{../pix/cc-attrib-nc.png}
\begin{minipage}[b]{3in}
{\tiny Licensed under the Creative Commons 
  attribution-noncommercial license
(\url{http://creativecommons.org/licenses/by-nc/3.0/}).
Please share \& remix noncommercially,
mentioning its origin.}
\end{minipage}

<<opts,echo=FALSE,message=FALSE,include=FALSE>>=
library("knitr")
do_tikz <- TRUE
opts_chunk$set(tidy=FALSE,fig.width=6,fig.height=4,fig.position="center",
               error=FALSE)
if (do_tikz) {
 opts_chunk$set(dev="tikz")
}
knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       par(bty="l",las=1)
                   } else { }
               })
@

\section{modeling}

\subsection{data analysis road map}

\begin{center}
\includegraphics[width=\textwidth]{../pix/mn_model.png} \\
\citep{McCullaghNelder1989}
\end{center}

\begin{center}
\includegraphics{../pix/tidy.png}
from Hadley Wickham
({\small \url{https://jules32.github.io/2016-07-12-Oxford/dplyr_tidyr/}})
\end{center}

These are good, but they don't address the \textbf{data snooping} problem.

\vskip5pt
\hrule
\vskip5pt
\parindent0pt
1. figure out the (subject-area) question \\
2. design experiment/data collection (power analysis; simulation)
\vskip5pt
\hrule
\vskip5pt
3. \emph{collect data} \\
\vskip5pt
\hrule
\vskip5pt
4. understand the data \\
5. specify the model; \emph{write it down!}
\vskip5pt
\hrule
\vskip5pt
6. inspect data (Q/A) (return to 5? \forkpath) \\
7. fit model \\
8. graphical \& quantitative diagnostics (return to 5? \forkpath) \\
9. interpret parameters; inference; plot results
\vskip5pt
\hrule
\vskip10pt

\section{basics}

Can use \emph{any} smooth function from $(0,1) \to \mathbb{R}$ as the link function
\begin{itemize}
  \item \emph{logistic} regression: binary data with a logit link (inverse-link=logistic)
  \item \emph{binomial} (or \emph{aggregated binomial} regression: binomial data (maybe logit link, maybe other)
  \item \emph{probit regression}: probit link
\end{itemize}
Binary data and aggregated ($N>1$ data) are handled slightly differently.

<<problog,echo=FALSE,message=FALSE,fig.width=6,fig.height=4>>=
library(ggplot2); theme_set(theme_bw())
library(directlabels)
linkdata <- data.frame(x=seq(-4,4,length=101))
linkvec <- c("logit","probit","cloglog")
scale <- c(1,sqrt(3)/pi,1)
for (i in 1:3) {
    linkdata[[linkvec[i]]] <- make.link(linkvec[i])$linkinv(linkdata$x*scale[i])
}
linkdata <- tidyr::gather(linkdata,invlink,prob,-x)
ggplot(linkdata,aes(x,prob,colour=invlink))+
    geom_line()+
   scale_colour_brewer(palette="Dark2")    
@

<<pkgs,message=FALSE>>=
library(ggplot2)
theme_set(theme_bw())
library(grid)
zmargin <- theme_update(panel.spacing=unit(0,"lines"))
library(dotwhisker)
library(descr)  ## for R^2 measures
library(aods3)  ## for overdispersion 
library(arm)    ## binnedplot
library(dplyr)  ## tidyverse!
library(DescTools)
library(broom)  ## for augment()
@ 

\section{Contraception data example}

<<contrac1,message=FALSE>>=
data("Contraception",package="mlmRev")
head(Contraception)
@ 

See \href{https://www.rdocumentation.org/packages/mlmRev/versions/1.0-6/topics/Contraception}{here} for more documentation.

\textbf{Given these variables, what model do we think we want to use?}

Visualize! Try some ggplots
(univariate graphs are OK but multivariate graphs are
almost always more informative \ldots)

<<ggplot_ex,fig.keep="none",message=FALSE>>=
gg0 <- ggplot(Contraception,aes(age,use,colour=urban))+
    stat_sum(alpha=0.5)+facet_wrap(~livch,labeller=label_both)
gg0 + geom_smooth(aes(group=1))
@ 

Hard to summarize 0/1 values!

Alternative approach: binning (also see Faraway). (Transform!)

<<aggbin>>=
## transform via tidyverse ...
cc <- (Contraception
    %>% mutate(
            ## numeric (0/1) version of 'uses contraception'
            use_n=as.numeric(use)-1)
)
cc_agg0 <- (cc
    %>% group_by(livch,urban,age)
    %>% summarise(prop=mean(use_n),
            n=length(use),
            se=sqrt(prop*(1-prop)/n))
)
@

Plot:

<<aggbin0plot,fig.keep="none">>=
ggplot(cc_agg0,aes(age,prop,colour=urban))+
    geom_pointrange(aes(ymin=prop-2*se,
                        ymax=prop+2*se))+
    facet_wrap(~livch,labeller=label_both)
@ 

Bin more coarsely:

<<aggbin2>>=
## specify categories; compute midpoints as well
age_breaks <- seq(-15,20,by=5)
age_mids <- (age_breaks[-1]+age_breaks[-length(age_breaks)])/2
cc_agg <- (cc
    ## discrete age categories            
    %>% mutate(age_cat=cut(age,breaks=age_breaks))
    %>% group_by(age_cat,urban,livch) 
    %>% summarise(
            prop=mean(use_n),
            n=length(use),
            se=sqrt(prop*(1-prop)/n)
        )
    ## numeric values of age categories
    %>% mutate(age_mid=age_mids[as.numeric(age_cat)])
)
@

Plot:

<<binplot>>=
## use numeric response rather than Y/N response
gg0B <- ggplot(cc,aes(age,use_n,colour=urban))+
    stat_sum(alpha=0.5)+facet_wrap(~livch,labeller=label_both)+
    labs(y="prob of contraceptive use")
gg_bin <- gg0B+geom_pointrange(data=cc_agg,
                    aes(x=age_mid,
                        y=prop,
                        ymin=prop-2*se,
                        ymax=prop+2*se,
                        size=n),
                    alpha=0.5)+
    scale_size(range=c(0.5,2))
@ 

\textbf{How should we adjust our model specification based on this information?}

\vskip10pt
\hrule
\vskip10pt

Suppose we use a model with a quadratic function of age plus all three-way interactions:

<<fit1>>=
model1 <- glm(use_n ~ urban*(age+I(age^2))*livch, 
              data=cc,
              family=binomial,
              x=TRUE  ## include model matrix in output
              )
@ 

Explore diagnostics (\code{plot()}; \code{DHARMa::simulateResiduals()};
\code{arm::binnedplot}; \code{mgcv::qq.gam}).

Q-Q plot is useless for logistic regression; we know that the responses are conditionally Bernoulli-distributed!
\textbf{Quantile residuals} \cite{ben_quantilequantile_2004,hartig_dharma_2018} overcome many of the problems of GLM diagnostics, at the price of lots more computation.

% dev="pdf" to avoid tikz problems with underscore 
% in default sub-caption to plot.lm() ...
<<diags,cache=TRUE,fig.keep="none",dev="pdf",message=FALSE>>=
## default plots: ugh!
plot(model1) 
## binned plot
arm::binnedplot(fitted(model1),residuals(model1))
## smoothing via ggplot
ggplot(broom::augment(model1),aes(.fitted,.resid)) +
    geom_point() + geom_smooth()
## Q-Q of quantile residuals
mgcv::qq.gam(model1,pch=1)
## ... simulated quantil residuals ...
mgcv::qq.gam(model1,pch=1,rep=1000)
## alternative simulated residuals
plot(DHARMa::simulateResiduals(model1),pch=".")

@ 

If you really need a global goodness-of-fit test:
\textbf{Hosmer-Lemeshow test} (very common) dominated by
Cessie-van Houwelingen test \cite{cessie_goodness_1991,hosmer_comparison_1997}.

<<hl,results="hide">>=
DescTools::HosmerLemeshowTest(fit=fitted(model1),
                              obs=model1$y,
                              X=model1$x)
@ 

\subsection{pseudo-$R^2$ measures}

The \href{http://statistics.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm}{UCLA statistics site} has a very nice description of pseudo-$R^2$ measures.

\begin{itemize}
\item fraction of variance explained
\item model improvement
\end{itemize}

\begin{itemize}
\item fraction of deviance explained: (dev(null)-dev(model))/dev(null)
  (``McFadden''):
<<mcfadden>>=
with(model1,1-deviance/null.deviance)
@ 
\item correlation (``Efron''):
<<efron>>=
cor(cc$use_n,predict(model1,type="response"))^2
@ 
\item Cox and Snell: average deviance explained
$$
1 - \left(L(\text{null})/L(\text{full})\right)^{2/n}
$$
(i.e. look at proportion on the likelihood scale, not the log-likelihood scale)
\item Nagelkerke: Cox and Snell, adjusted to max=1  
\end{itemize}

<<coxsnell,message=FALSE>>=
descr::LogRegR2(model1)
@ 

\subsection{Plot predictions}

<<plot_pred,fig.keep="none",warning=FALSE>>=
gg_bin + geom_smooth(method="glm",
                   method.args=list(family=binomial),
                   formula=y~x+I(x^2)
                   )
@ 

Or by hand: \code{predict} function.

Confidence intervals: get new model matrix and compute
$X V X^T$ to get variances on the link-function scale. Then
compute Normal CIs on the link scale, \emph{then} back-transform.
Or use \code{se=TRUE} in \code{predict}.
<<predci1,eval=FALSE>>=
pvar <- newX %*% vcov(g1) %*% t(newX)
pse <- sqrt(diag(pvar))
@

Or equivalently for any model type where \code{predict} has
an \code{se.fit} argument:
<<predci2,eval=FALSE>>=
pse <- predict(model,newdata=newdata,se.fit=TRUE)$se.fit
lwr <- plogis(pred0-2*pse)  ## or qnorm(0.025)
upr <- plogis(pred0+2*pse)  ## or qnorm(0.975)
@

Note:
\begin{itemize}
\item using the inverse-link function to back-transform the \emph{standard errors} never (??) makes sense: if you want to back-transform them (approximately), you have to multiply them by $(d \mu/d \eta)$, i.e. use \code{dlogis} or the \code{mu.eta} component of \verb!model$family!
\item if you use \code{response=TRUE} and \code{se.fit=TRUE},
  R computes the standard errors, scales them as above, and uses them to
  compute (approximate) \emph{symmetric} confidence intervals.  Unless your sample is very large and/or your predicted probabilities are near 0.5 (so the CIs don't approach 0 or 1), it's probably best to use the approach above
\end{itemize}

<<pframe>>=
## prediction frame: all combinations of variables
pframe <- with(Contraception,
               expand.grid(age=unique(age),
                           livch=levels(livch),
                           urban=levels(urban)))
predfun <- function(model) {
    pp <- predict(model, newdata=pframe, type="link", se.fit=TRUE)
    linkinv <- family(model)$linkinv
    pframe$use_n <- linkinv(pp$fit)
    pframe$lwr <- linkinv(pp$fit-2*pp$se.fit)
    pframe$upr <- linkinv(pp$fit+2*pp$se.fit)
    return(pframe)
}
pp1 <- predfun(model1)
@ 

\subsection{Posterior predictive simulations}

Pick a summary statistic that matters (e.g. the proportion
of urban women with no living children whose age is within
1 year of the mean who are using contraception) and simulate
predictions from the model: see how they match the observed
value.  Can we reject the null hypothesis that the model is OK?

<<ppred>>=
ppfun <- function(dd) {
    w <- which(dd$urban=="Y" & dd$livch=="0" & abs(dd$age)<1)
    return(mean(dd$use_n[w]))
}
ppfun(cc)  ## observed value from data
ss <- simulate(model1,1000)
simres <- rep(NA,1000)
newcc <- cc
for (i in 1:1000) {
    newcc$use_n <- ss[,i]
    simres[i] <- ppfun(newcc)
}
@ 

Plot results:

<<pphist>>=
par(las=1)
hist(simres,col="gray")
points(ppfun(cc),0,col="red",cex=2,pch=16)
p_upr <- mean(simres>=ppfun(cc))
p_lwr <- mean(simres<=ppfun(cc))
text(0.6,150,paste0("prop>=obs=",round(p_upr,2)))
## 2-tailed p-value
2*min(p_upr,p_lwr)
@ 

\subsection{Simplify model}

With caution!

<<simplify>>=
drop1(model1,test="Chisq")
## three-way interactions NS?
model2 <- update(model1, . ~ (urban+(age+I(age^2)+livch))^2)
drop1(model2,test="Chisq")
## two-way interactions NS?
model3 <- update(model1, . ~ (urban+(age+I(age^2)+livch)))
## or LRT
anova(model1,model2,model3,test="Chisq")
@ 


\subsection{Inference on the selected model}


<<modeltests,message=FALSE,warning=FALSE>>=
car::Anova(model3)
drop1(model3,test="Chisq")
@ 

Coefficient or ``dot-whisker'' plots
of the reduced model, with and without standardization
by $2\sigma$ of the predictors:

<<dwplot>>=
dw1 <- dwplot(model3)+geom_vline(xintercept=0,lty=2)
dw2 <- dwplot(model3,by_2sd=FALSE)+geom_vline(xintercept=0,lty=2)
@ 

Can compare the effect of dropping interactions (carefully!)
<<dwplot_interax>>=
mod_list <- list(full=model1,twoway=model2,reduced=model3)
dw_comb <- dwplot(mod_list)+ geom_vline(xintercept=0,lty=2)
@ 

<<compare_preds>>=
pp_list <- lapply(mod_list,predfun)
pp_frame <- dplyr::bind_rows(pp_list,.id="method")
gg_compare_pred <- gg0B + geom_line(data=pp_frame,
                                   aes(linetype=method))
pp3 <- pp_list[[3]]
gg_model3 <- gg0B + geom_line(data=pp3)+
    geom_ribbon(data=pp3,aes(ymin=lwr,ymax=upr,fill=urban),colour=NA,alpha=0.2)
@ 

<<coefs>>=
summary(model3)
@ 

The \code{emmeans} package has a whole bunch of convenience
functions for computing and plotting ``expected marginal means'',
which are the generalization of ``least-squares means'', i.e. 
effects averaged across categories in various sensible ways:

<<emmeansplot,warning=FALSE>>=
plot(emmeans::emmeans(model3,~livch*urban,type="response"))
@ 

\subsection{Confidence intervals on nonlinear functions of predictions}

Suppose we're interested in some value that can be computed
as a \emph{nonlinear} function of the parameters.  For example,
suppose want to estimate the age at which contraceptive use peaks,
and the level of contraception use at that point.
If a quadratic is parameterized as $\beta_0+\beta_1 x + \beta_2 x^2$, then
the critical values occurs where $\beta_1 + 2 \beta_2 \hat x = 0 \to \hat x=-\beta_1/(2\beta_2)$, and the value is $\beta_0 -\beta_1^2/(2\beta_2) + \beta_2 (\beta_1^2/(4 \beta_2^2)) = \beta_0 - \beta_1^2/(4 \beta_2)$. (Since the link function if monotonic, we don't have to
worry about that nonlinearity for these purposes.) Since we have only an additive model, 

<<peak_est>>=
cc3 <- as.list(coef(model3))
(use_peak <- with(cc3,
                  c(-age/(2*`I(age^2)`),
                    plogis(`(Intercept)` - age^2/(4*`I(age^2)`)))))
@
So the peak is half a year above the mean age, at about 
\Sexpr{round(100*use_peak[2])}\% use
(note that peak height varies among categories;
this is the prediction for the baseline category \{urban, livch=0\}).
These numbers seem reasonable based on what we've seen so far,
but checking graphically:
<<peak_graph1>>=
gg_model3+
    geom_vline(xintercept=use_peak[1],linetype=3)+
    geom_hline(yintercept=use_peak[2],linetype=3)
@

Getting the confidence intervals on these values is harder \ldots

\begin{itemize}
\item \textbf{delta method}:
If we want to compute the variance on the peak location?
of $f(x,y,z)$ and ${\bm g}=(\frac{\partial f}{\partial x},
\frac{\partial f}{\partial y},
\frac{\partial f}{\partial z})$ then the variance is
${\bm g} V {\bm g}^T$ (which reduces to 
$\text{CV}^2(f(x,y)) = \text{CV}^2(x)+\text{CV}^2(y)$
for the case of independent values when $f(x,y)=x/y$
or $xy$):
<<peakgrad2>>=
grad <- rep(0,length(coef(model3)))
names(grad) <- names(coef(model3))
## deriv of b1/(2*b2) = {1/(2*b2), -b1/b2^2}
grad[c("age","I(age^2)")] <- 
    with(cc3,c(1/(2*`I(age^2)`),-age/`I(age^2)`^2))
peak_var <- t(grad) %*% vcov(model3) %*% grad
peak_se <- c(sqrt(peak_var)) ## c() converts from matrix to vector (= scalar)
deltaCI <- use_peak[1]+c(-1,1)*2*peak_se
@ 

Plot:
<<plot_peakgrad2>>=
gg_model3+geom_vline(xintercept=use_peak[1],linetype=3)+
    annotate("rect",
           xmin=deltaCI[1],
           xmax=deltaCI[2],
           ymin=-Inf,
           ymax=Inf,alpha=0.3,fill="blue",
             colour=NA)
@
\item bootstrapping
<<boot,cache=TRUE>>=
bootres <- numeric(250)
for (i in 1:250) {
    bootdat <- cc[sample(nrow(cc),replace=TRUE),]
    bootmodel <- update(model3,data=bootdat)
    bootcc <- coef(bootmodel)
    bootres[i] <- with(as.list(bootcc),c(age/(2*`I(age^2)`)))
}
hist(bootres,col="gray",breaks=50)
bootCI <- quantile(bootres,c(0.025,0.975))
@
\item pseudo-Bayes: MVN sample from parameters
<<pseudobayes>>=
library(MASS)
PBsamp <- as.data.frame(mvrnorm(1000,mu=coef(model3),Sigma=vcov(model3)))
PBres <- with(PBsamp,c(age/(2*`I(age^2)`)))
hist(PBres,col="gray",breaks=50)
PBCI <- quantile(PBres,c(0.025,0.975))
@
\end{itemize}

In this case the results are all extremely similar:

<<cfres>>=
rbind(deltaCI,PBCI,bootCI)
@

\bibliography{../glmm}
\end{document}

