\documentclass{tufte-handout}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\hypersetup{colorlinks,linkcolor=blue}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel} %% texi2dvi ~ bug
\usepackage{tikz} % http://www.texample.net/tikz/examples/tikzdevice-demo/
\usepackage{natbib}
\usepackage{bm}
\newcommand{\code}[1]{{\tt #1}}
\newcommand{\y}{{\mathbf y}}
\newcommand{\Y}{{\mathbf Y}}
\newcommand{\V}{{\mathbf V}}
\newcommand{\bb}{{\mathbf b}}
\newcommand{\bbeta}{{\bm \beta}}
\newcommand{\bmu}{{\bm \mu}}
\newcommand{\X}{\mathbf X}
\newcommand{\forkpath}{\raisebox{-3pt}{\includegraphics[width=0.4cm]{../pix/fp_icon.png}}}

\title{Logistic and binomial regression}
\author{Ben Bolker}
\begin{document}
\maketitle
\bibliographystyle{chicago}

\includegraphics[width=2.64cm,height=0.93cm]{../pix/cc-attrib-nc.png}
\begin{minipage}[b]{3in}
{\tiny Licensed under the Creative Commons 
  attribution-noncommercial license
(\url{http://creativecommons.org/licenses/by-nc/3.0/}).
Please share \& remix noncommercially,
mentioning its origin.}
\end{minipage}

<<opts,echo=FALSE,message=FALSE,include=FALSE>>=
library("knitr")
do_tikz <- FALSE
opts_chunk$set(tidy=FALSE,fig.width=6,fig.height=4,fig.position="center",
               error=TRUE)
if (do_tikz) {
 opts_chunk$set(dev="tikz")
}
knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       par(bty="l",las=1)
                   } else { }
               })
@

\section{modeling}

\subsection{data analysis road map}

\begin{center}
\includegraphics[width=\textwidth]{../pix/mn_model.png} \\
\citep{McCullaghNelder1989}
\end{center}

\begin{center}
\includegraphics{../pix/tidy.png}
from Hadley Wickham
({\small \url{https://jules32.github.io/2016-07-12-Oxford/dplyr_tidyr/}})
\end{center}

These are good, but they don't address the \textbf{data snooping} problem.

\vskip5pt
\hrule
\vskip5pt
\parindent0pt
1. figure out the (subject-area) question \\
2. design experiment/data collection (power analysis; simulation)
\vskip5pt
\hrule
\vskip5pt
3. \emph{collect data} \\
\vskip5pt
\hrule
\vskip5pt
4. understand the data \\
5. specify the model; \emph{write it down!}
\vskip5pt
\hrule
\vskip5pt
6. inspect data (Q/A) (return to 5? \forkpath) \\
7. fit model \\
8. graphical \& quantitative diagnostics (return to 5? \forkpath) \\
9. interpret parameters; inference; plot results
\vskip5pt
\hrule
\vskip10pt

\section{basics}

Can use \emph{any} smooth function from $(0,1) \to \mathbb{R}$ as the link function
\begin{itemize}
  \item \emph{logistic} regression: binary data with a logit link (inverse-link=logistic)
  \item \emph{binomial} (or \emph{aggregated binomial} regression: binomial data (maybe logit link, maybe other)
  \item \emph{probit regression}: probit link
\end{itemize}
Binary data and aggregated ($N>1$ data) are handled slightly differently.

<<problog,echo=FALSE,message=FALSE,fig.width=6,fig.height=4>>=
library(ggplot2); theme_set(theme_bw())
library(directlabels)
linkdata <- data.frame(x=seq(-4,4,length=101))
linkvec <- c("logit","probit","cloglog")
scale <- c(1,sqrt(3)/pi,1)
for (i in 1:3) {
    linkdata[[linkvec[i]]] <- make.link(linkvec[i])$linkinv(linkdata$x*scale[i])
}
linkdata <- tidyr::gather(linkdata,invlink,prob,-x)
ggplot(linkdata,aes(x,prob,colour=invlink))+
    geom_line()+
   scale_colour_brewer(palette="Dark2")    
@

<<pkgs,message=FALSE>>=
library(ggplot2)
theme_set(theme_bw())
library(grid)
zmargin <- theme_update(panel.spacing=unit(0,"lines"))
library(dotwhisker)
library(descr)  ## for R^2 measures
library(aods3)  ## for overdispersion 
library(arm)    ## binnedplot
library(dplyr)  ## tidyverse!
library(DescTools)
@ 

\section{Contraception data example}

<<contrac1,message=FALSE>>=
data("Contraception",package="mlmRev")
head(Contraception)
@ 

See \href{https://www.rdocumentation.org/packages/mlmRev/versions/1.0-6/topics/Contraception}{here} for more documentation.

\textbf{Given these variables, what model do we think we want to use?}

Visualize! Try some ggplots
(univariate graphs are OK but multivariate graphs are
almost always more informative \ldots)

<<ggplot_ex,fig.keep="none",message=FALSE>>=
gg0 <- ggplot(Contraception,aes(age,use,colour=urban))+
    stat_sum(alpha=0.5)+facet_wrap(~livch,labeller=label_both)
gg0 + geom_smooth(aes(group=1))
@ 

Hard to summarize 0/1 values!

Alternative approach: binning (also see Faraway). (Transform!)

<<aggbin>>=
## transform via tidyverse ...
cc <- (Contraception
    %>% mutate(
            ## numeric (0/1) version of 'uses contraception'
            use_n=as.numeric(use)-1)
)
cc_agg0 <- (cc
    %>% group_by(livch,urban,age)
    %>% summarise(prop=mean(use_n),
            n=length(use),
            se=sqrt(prop*(1-prop)/n))
)
@

Plot:

<<aggbin0plot,fig.keep="none">>=
ggplot(cc_agg0,aes(age,prop,colour=urban))+
    geom_pointrange(aes(ymin=prop-2*se,
                        ymax=prop+2*se))+
    facet_wrap(~livch,labeller=label_both)
@ 

Bin more coarsely:

<<aggbin2>>=
## specify categories; compute midpoints as well
age_breaks <- seq(-15,20,by=5)
age_mids <- (age_breaks[-1]+age_breaks[-length(age_breaks)])/2
            ## discrete age categories            
            age_cat=cut(age,breaks=age_breaks))
)
cc_agg <- (cc
    %>% mutate(age_cat=cut(age,breaks=age_breaks))
    %>% group_by(age_cat,urban,livch) 
    %>% summarise(
            prop=mean(use_n),
            n=length(use),
            se=sqrt(prop*(1-prop)/n)
        )
    ## numeric values of age categories
    %>% mutate(age_mid=age_mids[as.numeric(age_cat)])
)
@

Plot:

<<binplot>>=
## use numeric response rather than Y/N response
gg0B <- ggplot(cc,aes(age,use_n,colour=urban))+
    stat_sum(alpha=0.5)+facet_wrap(~livch,labeller=label_both)
gg_bin <- gg0B+geom_pointrange(data=cc_agg,
                    aes(x=age_mid,
                        y=prop,
                        ymin=prop-2*se,
                        ymax=prop+2*se,
                        size=n),
                    alpha=0.5)+
    scale_size(range=c(0.5,2))
@ 

\textbf{How should we adjust our model specification based on this information?}

\vskip10pt
\hrule
\vskip10pt

Suppose we use a model with a quadratic function of age plus all three-way interactions:

<<fit1>>=
model1 <- glm(use_n ~ urban*(age+I(age^2))*livch, 
              data=cc,
              family=binomial,
              x=TRUE  ## include model matrix in output
              )
@ 

Explore diagnostics (\code{plot()}; \code{DHARMa::simulateResiduals()};
\code{arm::binnedplot}; \code{mgcv::qq.gam}).

\textbf{Quantile residuals} \cite{ben_quantilequantile_2004,hartig_dharma_2018} overcome many of the problems of GLM diagnostics, at the price of lots more computation.

<<diags,eval=FALSE>>=
plot(model1)  ## ugh!
arm::binnedplot(fitted(model1),residuals(model1))
DHARMa::simulateResiduals(model1,plot=TRUE)
mgcv::qq.gam(model1,pch=1)
@ 

If you really need a global goodness-of-fit test:
\textbf{Hosmer-Lemeshow test} (very common) dominated by
Cessie-van Houwelingen test \cite{cessie_goodness_1991,hosmer_comparison_1997}.

<<hl,results="hide">>=
DescTools::HosmerLemeshowTest(fit=fitted(model1),
                              obs=model1$y,
                              X=model1$x)
@ 

\subsection{pseudo-$R^2$ measures}

The \href{http://statistics.ats.ucla.edu/stat/mult_pkg/faq/general/Psuedo_RSquareds.htm}{UCLA statistics site} has a very nice description of pseudo-$R^2$ measures.

\begin{itemize}
\item fraction of variance explained
\item model improvement
\end{itemize}

\begin{itemize}
\item fraction of deviance explained: (dev(null)-dev(model))/dev(null)
  (``McFadden''):
<<mcfadden>>=
with(model3,1-deviance/null.deviance)
@ 
\item correlation (``Efron''):
<<efron>>=
cor(cc$use_n,predict(model3,type="response"))^2
@ 
\item Cox and Snell: average deviance explained
$$
1 - \left(L(\text{null})/L(\text{full})\right)^{2/n}
$$
(i.e. look at proportion on the likelihood scale, not the log-likelihood scale)
\item Nagelkerke: Cox and Snell, adjusted to max=1  
\end{itemize}

<<coxsnell,message=FALSE>>=
descr::LogRegR2(model3)
@ 

\subsection{Plot predictions}

<<warning=FALSE>>=
gg_bin+geom_smooth(method="glm",
                   method.args=list(family=binomial),
                   formula=y~x+I(x^2)
                   )
@ 

Or by hand: \code{predict} function.

Confidence intervals: get new model matrix and compute
$X V X^T$ to get variances on the link-function scale. Then
compute Normal CIs on the link scale, \emph{then} back-transform.
Or use \code{se=TRUE} in \code{predict}.
<<predci1>>=
pvar <- newX %*% vcov(g1) %*% t(newX)
pse <- sqrt(diag(pvar))
@

Or equivalently for any model type where \code{predict} has
an \code{se.fit} argument:
<<predci2>>=
pse <- predict(g1,newdata=newdata,se.fit=TRUE)$se.fit
lwr0 <- pred0-2*pse  ## or qnorm(0.025)
upr0 <- pred0+2*pse  ## or qnorm(0.975)
lwr <- plogis(lwr0)
upr <- plogis(upr0)
@

Note:
\begin{itemize}
\item back-transforming the standard errors via a logistic usually doesn't make sense: if you want to back-transform them (approximately), you have to multiply them by $(d \mu/d \eta)$, i.e. use \code{dlogis}.
\item if you use \code{response=TRUE} and \code{se.fit=TRUE},
  R computes the standard errors, scales them as above, and uses them to
  compute (approximate) \emph{symmetric} confidence intervals.  Unless your sample is very large and/or your predicted probabilities are near 0.5 (so the CIs don't get near 0 or 1), it's probably best to use the approach above
\end{itemize}

<<pframe>>=
## prediction frame: all combinations of variables
pframe <- with(Contraception,
               expand.grid(age=unique(age),
                           livch=levels(livch),
                           urban=levels(urban)))
predfun <- function(model) {
    pp <- predict(model, newdata=pframe, type="link", se.fit=TRUE)
    linkinv <- family(model)$linkinv
    pframe$use_n <- linkinv(pp$fit)
    pframe$lwr <- linkinv(pp$fit-2*pp$se.fit)
    pframe$upr <- linkinv(pp$fit+2*pp$se.fit)
    return(pframe)
}
pp1 <- predfun(model1)
@ 

\subsection{Posterior predictive simulations}

Pick a summary statistic that matters (e.g.

<<ppred>>=
ppfun <- function(dd) {
    w <- which(dd$urban=="Y" & dd$livch=="0" & abs(dd$age)<1)
    return(mean(dd$use_n[w]))
}
ppfun(cc)
ss <- simulate(model1,1000)
simres <- rep(NA,1000)
newcc <- cc
for (i in 1:1000) {
    newcc$use_n <- ss[,i]
    simres[i] <- ppfun(newcc)
}
@ 

Plot results:

<<pphist>>=
par(las=1)
hist(simres,col="gray")
points(ppfun(cc),0,col="red",cex=2,pch=16)
p_upr <- mean(simres>=ppfun(cc))
p_lwr <- mean(simres<=ppfun(cc))
text(0.6,150,paste0("prop>=obs=",round(p_upr,2)))
## 2-tailed p-value
2*min(p_upr,p_lwr)
@ 

\subsection{Simplify model}

With caution!

<<simplify>>=
drop1(model1,test="Chisq")
## three-way interactions NS?
model2 <- update(model1, . ~ (urban+(age+I(age^2)+livch))^2)
drop1(model2,test="Chisq")
## two-way interactions NS?
model3 <- update(model1, . ~ (urban+(age+I(age^2)+livch)))
## or LRT
anova(model1,model2,model3,test="Chisq")
@ 


\subsection{}

<<modeltests,message=FALSE>>=
car::Anova(model3)
drop1(model3,test="Chisq")
@ 
<<dwplot>>=
dw1 <- dwplot(model3)+geom_vline(xintercept=0,lty=2)
dw2 <- dwplot(model3,by_2sd=FALSE)+geom_vline(xintercept=0,lty=2)
@ 

Can compare the effect of dropping interactions (carefully!)
<<dwplot_interax>>=
mod_list <- list(full=model1,twoway=model2,reduced=model3)
dw_comb <- dwplot(mod_list)+ geom_vline(xintercept=0,lty=2)
@ 

<<compare_preds>>=
pp_list <- lapply(mod_list,predfun)
pp_frame <- dplyr::bind_rows(pp_list,.id="method")
gg_compare_pred <- gg0 + geom_line(data=pp_frame,
                                   aes(linetype=method))
@ 

<<coefs>>=
summary(model3)
@ 

<<emmeansplot,warning=FALSE>>=
plot(emmeans::emmeans(model3,~livch*urban,type="response"))
@ 

\subsection{Confidence intervals on predictions etc.}

(delta method; bootstrap; simulation)

\bibliography{../glmm}
\end{document}

